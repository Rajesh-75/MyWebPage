<!DOCTYPE html>
<html lang="en">
<head>
Â  Â  <meta charset="UTF-8">
Â  Â  <meta name="viewport" content="width=device-width, initial-scale=1.0">
Â  Â  <title>Introduction to ML</title>
Â  Â  <style>
Â  Â  Â  Â  /* Basic styling for demonstration and readability */
Â  Â  Â  Â  body {
Â  Â  Â  Â  Â  Â  font-family: sans-serif;
Â  Â  Â  Â  Â  Â  line-height: 1.6;
Â  Â  Â  Â  Â  Â  margin: 20px;
Â  Â  Â  Â  Â  Â  padding: 0;
Â  Â  Â  Â  Â  Â  color: #333;
Â  Â  Â  Â  }
Â  Â  Â  Â  h1, h2 {
Â  Â  Â  Â  Â  Â  color: #0056b3;
Â  Â  Â  Â  }
Â  Â  Â  Â  .detail-title {
Â  Â  Â  Â  Â  Â  border-bottom: 2px solid #ddd;
Â  Â  Â  Â  Â  Â  padding-bottom: 10px;
Â  Â  Â  Â  }
Â  Â  Â  Â  .image-title {
Â  Â  Â  Â  Â  Â  text-align: center;
Â  Â  Â  Â  Â  Â  font-style: italic;
Â  Â  Â  Â  Â  Â  margin-top: 5px;
Â  Â  Â  Â  Â  Â  margin-bottom: 20px;
Â  Â  Â  Â  Â  Â  color: #555;
Â  Â  Â  Â  Â  Â  font-size: 0.9em;
Â  Â  Â  Â  }
        .pipeline-image {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border: 1px solid #ccc;
            border-radius: 5px;
        }
Â  Â  </style>
</head>
<body>

Â  Â  <h1 class="detail-title">AI and Statistics</h1>
Â  Â  <p class="detail-description">AI (Artificial Intelligence) is one of the most important and fastest-growing fields of technology today. Applications of AI are becoming ubiquitous, and solutions learned from data are increasingly displacing traditional hand-crafted algorithms. Machine Learning (ML) and Deep Learning (DL) are major branches of artificial intelligence (AI) that provide systems with the ability to automatically learn and improve from experience without being explicitly programmed.</p>
Â  Â  Â 
Â  Â  <p class="detail-description">This tutorial explores the fundamental theory and practical engineering of various Machine Learning and Deep Learning models, including classic algorithms like SVM, Decision Trees, and Naive Bayesian, as well as advanced architectures such as Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and Transformers. We will also demonstrate how to train and apply these models across diverse real-world applications.</p>
Â  Â Â 
Â  Â  <p class="detail-description">Serving as the major foundational tool, Statistics underpins the entire scope of this tutorial.</p>
Â  Â Â 
Â  Â  <p class="detail-description">Both Machine Learning and Deep Learning follow a pipeline structure to create and deploy AI-based systems. The pipeline commonly has the following form:</p>
Â  Â  Â 
Â  Â  <img src="image_fdb59d.png" alt="Machine Learning Pipeline: Data Ingestion, Preprocessing, Feature Engineering, Model Training, Evaluation, Deployment" class="pipeline-image">
Â  Â  <p class="image-title">ML Pipeline Flow</p>

Â  Â  <img src="NNbasedPipelineforImageRecongition.png" alt="Neural Network based Pipeline for Image Recognition" class="pipeline-image">Â 
Â  Â  <p class="image-title">An example pipeline based on Neural Network for Image Recognition</p>

Â  Â  <h2 style="margin-top: 30px;">The ML Pipeline</h2>
Â  Â  <ul>
Â  Â  Â  Â  <li>Data Ingestion: Gathering and loading data from various sources (databases, APIs, files).</li>
Â  Â  Â  Â  <li>Preprocessing: Cleaning the data, handling missing values, and dealing with inconsistencies.</li>
Â  Â  Â  Â  <li>Feature Engineering: Transforming raw data into features that better represent the underlying problem to the predictive models.</li>
Â  Â  Â  Â  <li>Model Training: Feeding the prepared data to the chosen algorithm (like Decision Trees or SVM) to learn patterns.</li>
Â  Â  Â  Â  <li>Evaluation: Testing the model's performance on unseen data to ensure accuracy and robustness.</li>
Â  Â  Â  Â  <li>Deployment: Integrating the model into a live environment where it can make real-time predictions.</li>
Â  Â  </ul>

    <!-- New Section Added Below -->
    <h2 style="margin-top: 40px;">Data Ingestion</h2>
	<p class="detail-description">
	Progress in the last decade shows that the success of an ML system depends
largely on the data it was trained on. Instead of focusing on improving ML
algorithms, most companies focus on managing and improving their data.
</p>

    <p class="detail-description">
        Data Ingestion, the first stage of the ML pipeline, involves a structured process of acquiring raw data and preparing its storage environment. This sub-pipeline ensures data is collected reliably and organized efficiently for subsequent processing.
    </p>

    <img src="DataStorage.png" alt="Data Engineering Sub-pipeline showing Data Acquisition (Finding Data Sources, Ingestion Modes, Validation Schema) leading to Data Storage and Governance (Data Lake/Data Warehouse/Feature Store, Data Governance/Security)" class="pipeline-image">
    <p class="image-title">Data Ingestion Sub-Pipeline (Acquisition and Storage Stages)</p>
	<hr>
    <h2 style="margin-top: 40px;">The Data Engineering Sub-Pipeline for ML/DL</h2>
    <p class="detail-description">The Data Engineering pipeline is responsible for transforming raw, disparate data into the clean, reliable, and accessible features required for model training and inference. It generally follows a structured ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform) pattern, adapted for machine learning needs.</p>

    

    <h3 style="margin-top: 20px;">Stage 1: Data Acquisition & Ingestion (The "Extract" Phase)</h3>
    <p class="detail-description">This initial stage focuses on finding, collecting, and moving raw data from various sources into a central landing zone.</p>
    
    <h4>1. Finding Data Sources</h4>
    <ul>
        <li>Internal Sources: Databases (OLTP/Transactional, Data Warehouses), Application logs, Sensor data, CRM systems, or existing data lakes.</li>
        <li>External Sources: Third-party APIs, public datasets, web scraping, or syndicated data feeds.</li>
		<li>Specific Data Types (e.g., Image/Video): Sources for unstructured data like images and video often include large public repositories (e.g., ImageNet, COCO, Open Images), proprietary camera feeds, or custom annotation/labeling services for specific domains.</li>
</ul>
    </ul>

    <h4>2. Ingestion Modes</h4>
    <ul>
        <li>Batch Ingestion: Processing large volumes of data at scheduled intervals (e.g., nightly jobs, weekly loads). Suitable for static datasets like historical sales or user demographics.</li>
        <li>Streaming Ingestion: Continuous, real-time processing of data as it's generated (e.g., stock market trades, website clickstreams, sensor readings). Requires technologies like Kafka or Pub/Sub.</li>
    </ul>

    <h4>3. Validation & Schema Check</h4>
    <p class="detail-description">As data enters the pipeline, basic validation ensures the data type and format conform to the expected schema (Schema-on-Read vs. Schema-on-Write).</p>
    
    <h3 style="margin-top: 20px;">Stage 2: Data Storage & Governance (The "Load" Phase)</h3>
    <p class="detail-description">This stage determines where the raw and processed data resides, optimized for both cost and query performance.</p>
    
    <h4>1. Storage Solutions</h4>
    <ul>
        <li>Data Lake: Stores raw, uncleaned, and unstructured data (e.g., images, logs, text documents) in its native format, often on cloud object storage (S3, GCS). This is typically the initial landing zone.</li>
        <li>Data Warehouse: Stores structured, cleaned data optimized for analytical querying (OLAP). Ideal for structured features and aggregate data used in traditional ML.</li>
        <li>Feature Store: A specialized data system designed specifically for ML. It stores and serves curated feature vectors consistently for both training (batch access) and real-time inference (low-latency lookup).</li>
    </ul>

    <h4>2. Data Governance and Security</h4>
    <p class="detail-description">Implementing access controls, encryption, and anonymization/pseudonymization to ensure compliance with regulations (GDPR, HIPAA). Managing data lineageâ€”tracking where data came from and all transformations applied.</p>
<hr>
<h2 style="margin-top: 40px;">Data Pre-processing</h2>
<p class="detail-description">Data pre-processing is the critical stage that cleans, transforms, and prepares the raw data to be suitable for model training. It directly impacts model performance and stability.</p>


<img src="DataPreProcessing.jpg" alt="Data Pre-Processing" class="pipeline-image">


<h3 style="margin-top: 20px;">1. Data Cleaning ðŸ§¹</h3>
<p class="detail-description">This step focuses on fixing errors and inconsistencies in the data.</p>
<ul>
    <li>Handling Missing Values: Identifying data points where values are absent and either:
        <ul>
            <li>Imputing: Replacing them with a calculated value (e.g., the mean, median, or mode).</li>
            <li>Deleting: Removing rows or columns that have too many missing values.</li>
        </ul>
    </li>
    <li>Handling Noisy Data: Correcting or smoothing out random error or variance in the data, which can result from collection errors or incorrect labeling.</li>
    <li>Outlier Detection and Treatment: Identifying data points that significantly deviate from the majority and deciding whether to remove them, cap them, or transform them, as they can disproportionately affect model training.</li>
    <li>Dealing with Inconsistent Data: Correcting structural errors and naming inconsistencies (e.g., 'NY' vs. 'New York') or incorrect data types.</li>
</ul>

<h3 style="margin-top: 20px;">2. Data Transformation ðŸ”„</h3>
<p class="detail-description">This step changes the format, scale, or distribution of the data into a numerical format that ML algorithms can process efficiently.</p>
<ul>
    <li>Normalization and Scaling: Adjusting the range of feature values to a standard scale.
        <ul>
            <li>Normalization (Min-Max): Scales values to a fixed range, typically [0, 1].</li>
            <li>Standardization (Z-score): Scales data to have a mean of 0 and a standard deviation of 1.</li>
        </ul>
    </li>
    <li>Encoding Categorical Data: Converting non-numeric (textual) labels into numerical formats.
        <ul>
            <li>One-Hot Encoding: Creates new binary columns for each category.</li>
            <li>Label Encoding: Assigns a unique integer to each category.</li>
        </ul>
    </li>
    <li>Discretization: Converting continuous numerical data into a finite set of intervals or bins (e.g., replacing exact age with 'young', 'middle-aged', 'senior').</li>
</ul>

<h3 style="margin-top: 20px;">3. Data Reduction ðŸ“‰</h3>
<p class="detail-description">This step aims to obtain a reduced representation of the data that is smaller in volume but still produces the same analytical results, improving efficiency.</p>
<ul>
    <li>Dimensionality Reduction: Reducing the number of features (columns).
        <ul>
            <li>Feature Selection: Choosing a subset of the most relevant features.</li>
            <li>Feature Extraction: Transforming data from a high-dimensional space to a lower one (e.g., using Principal Component Analysis (PCA)).</li>
        </ul>
    </li>
    <li>Data Compression: Encoding data to reduce storage space.</li>
    <li>Numerosity Reduction: Replacing actual data values with alternative, smaller representations (e.g., regression or clustering).</li>
</ul>	

</body>
</html>