<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to ML</title>
    <style>
        /* Basic styling for demonstration and readability */
        body {
            font-family: sans-serif;
            line-height: 1.6;
            margin: 20px;
            padding: 0;
            color: #333;
        }
        h1, h2 {
            color: #0056b3;
        }
        .detail-title {
            border-bottom: 2px solid #ddd;
            padding-bottom: 10px;
        }
        .image-title {
            text-align: center;
            font-style: italic;
            margin-top: 5px;
            margin-bottom: 20px;
            color: #555;
            font-size: 0.9em;
        }
        .pipeline-image {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border: 1px solid #ccc;
            border-radius: 5px;
        }
    </style>
</head>
<body>

    <h1 class="detail-title">AI and Statistics</h1>
    <p class="detail-description">AI (Artificial Intelligence) is one of the most important and fastest-growing fields of technology today. Applications of AI are becoming ubiquitous, and solutions learned from data are increasingly displacing traditional hand-crafted algorithms. Machine Learning (ML) and Deep Learning (DL) are major branches of artificial intelligence (AI) that provide systems with the ability to automatically learn and improve from experience without being explicitly programmed.</p>
     
    <p class="detail-description">This tutorial explores the fundamental theory and practical engineering of various Machine Learning and Deep Learning models, including classic algorithms like SVM, Decision Trees, and Naive Bayesian, as well as advanced architectures such as Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and Transformers. We will also demonstrate how to train and apply these models across diverse real-world applications.</p>
    
    <p class="detail-description">Serving as the **major foundational tool**, Statistics underpins the entire scope of this tutorial.</p>
    
    <p class="detail-description">Both Machine Learning and Deep Learning follow a pipeline structure to create and deploy AI-based systems. The pipeline commonly has the following form:</p>
     
    <img src="image_fdb59d.png" alt="Machine Learning Pipeline: Data Ingestion, Preprocessing, Feature Engineering, Model Training, Evaluation, Deployment" class="pipeline-image">
    <p class="image-title">ML Pipeline Flow</p>

    <img src="NNbasedPipelineforImageRecongition.png" alt="Neural Network based Pipeline for Image Recognition" class="pipeline-image"> 
    <p class="image-title">An example pipeline based on Neural Network for Image Recognition</p>

    <h2 style="margin-top: 30px;">The ML Pipeline</h2>
    <ul>
        <li>**Data Ingestion:** Gathering and loading data from various sources (databases, APIs, files).</li>
        <li>**Preprocessing:** Cleaning the data, handling missing values, and dealing with inconsistencies.</li>
        <li>**Feature Engineering:** Transforming raw data into features that better represent the underlying problem to the predictive models.</li>
        <li>**Model Training:** Feeding the prepared data to the chosen algorithm (like Decision Trees or SVM) to learn patterns.</li>
        <li>**Evaluation:** Testing the model's performance on unseen data to ensure accuracy and robustness.</li>
        <li>**Deployment:** Integrating the model into a live environment where it can make real-time predictions.</li>
    </ul>

    <!-- New Section Added Below -->
    <h2 style="margin-top: 40px;">Data Ingestion</h2>
    <p class="detail-description">
        Data Ingestion, the first stage of the ML pipeline, involves a structured process of acquiring raw data and preparing its storage environment. This sub-pipeline ensures data is collected reliably and organized efficiently for subsequent processing.
    </p>

    <img src="DataStorage.png" alt="Data Engineering Sub-pipeline showing Data Acquisition (Finding Data Sources, Ingestion Modes, Validation Schema) leading to Data Storage and Governance (Data Lake/Data Warehouse/Feature Store, Data Governance/Security)" class="pipeline-image">
    <p class="image-title">Data Ingestion Sub-Pipeline (Acquisition and Storage Stages)</p>
	<hr>
    <h2 style="margin-top: 40px;">The Data Engineering Sub-Pipeline for ML/DL</h2>
    <p class="detail-description">The **Data Engineering pipeline** is responsible for transforming raw, disparate data into the clean, reliable, and accessible features required for model training and inference. It generally follows a structured **ETL** (Extract, Transform, Load) or **ELT** (Extract, Load, Transform) pattern, adapted for machine learning needs.</p>

    

    <h3 style="margin-top: 20px;">Stage 1: Data Acquisition & Ingestion (The "Extract" Phase)</h3>
    <p class="detail-description">This initial stage focuses on finding, collecting, and moving raw data from various sources into a central landing zone.</p>
    
    <h4>1. Finding Data Sources</h4>
    <ul>
        <li>**Internal Sources:** Databases (OLTP/Transactional, Data Warehouses), Application logs, Sensor data, CRM systems, or existing data lakes.</li>
        <li>**External Sources:** Third-party APIs, public datasets, web scraping, or syndicated data feeds.</li>
    </ul>

    <h4>2. Ingestion Modes</h4>
    <ul>
        <li>**Batch Ingestion:** Processing large volumes of data at scheduled intervals (e.g., nightly jobs, weekly loads). Suitable for static datasets like historical sales or user demographics.</li>
        <li>**Streaming Ingestion:** Continuous, real-time processing of data as it's generated (e.g., stock market trades, website clickstreams, sensor readings). Requires technologies like Kafka or Pub/Sub.</li>
    </ul>

    <h4>3. Validation & Schema Check</h4>
    <p class="detail-description">As data enters the pipeline, basic validation ensures the data type and format conform to the expected schema (Schema-on-Read vs. Schema-on-Write).</p>
    
    <h3 style="margin-top: 20px;">Stage 2: Data Storage & Governance (The "Load" Phase)</h3>
    <p class="detail-description">This stage determines where the raw and processed data resides, optimized for both cost and query performance.</p>
    
    <h4>1. Storage Solutions</h4>
    <ul>
        <li>**Data Lake:** Stores raw, uncleaned, and unstructured data (e.g., images, logs, text documents) in its native format, often on cloud object storage (S3, GCS). This is typically the initial landing zone.</li>
        <li>**Data Warehouse:** Stores structured, cleaned data optimized for analytical querying (OLAP). Ideal for structured features and aggregate data used in traditional ML.</li>
        <li>**Feature Store:** A specialized data system designed specifically for ML. It stores and serves curated feature vectors consistently for both training (batch access) and real-time inference (low-latency lookup).</li>
    </ul>

    <h4>2. Data Governance and Security</h4>
    <p class="detail-description">Implementing access controls, encryption, and anonymization/pseudonymization to ensure compliance with regulations (GDPR, HIPAA). Managing data lineage—tracking where data came from and all transformations applied.</p>

</body>
</html>