<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <link rel="stylesheet" href="style.css">
    <title>Machine Learning Tutorial</title>
    <script type="text/javascript" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\(', '\)']],
                displayMath: [['$$', '$$'], ['\[', '\]']]
            }
        };
    </script>
</head>
<body>

<div class="container">
    <h1>Random Variable and Expectation</h1>

    <h2>Random variable</h2>
    <p>A real valued function defined over a sample space of random experiment is called the random variable. A variable which takes corresponding to the outcomes of a random experiment is called random variable.</p>

    <div class="example-box">
        <strong>eg (1)</strong> In the experiment of throwing a die, the outcomes are 1,2,3,4,5,6 so the random variable of getting number takes values 1,2,3,4,5,6,7 <br><br>
        <strong>(2)</strong> In the case of tossing 3 coins the outcome are getting 0 head, 1 head, 2 heads and 3 head. So the random variable of getting heads takes the value 0,1,2,3.
    </div>

    <h2>Distribution of a random variable (Probability Distribution)</h2>
    <p>Let 'X' be a random variable. Let X assume values as $x_1, x_2, x_3 \dots$</p>
    <p>The probability that X takes any x ($x_1$ or $x_2$ or $x_3 \dots$) is given by $P(X = x)$. It can also be denoted by $f(x)$.</p>
    <p><strong>Probability Density</strong> is the distribution of probabilities with respect to its random variable. These distributions are classified into two main types:</p>

    <div style="display: flex; gap: 20px; margin: 20px 0;">
        <div style="flex: 1; background: white; padding: 15px; border: 1px solid #ddd; border-radius: 5px;">
            <h4 style="color: #2980b9;">1. Discrete Distribution</h4>
            <p>Takes on <strong>individual, countable values</strong> (e.g., 0, 1, 2 heads). Probabilities are assigned to each specific point.</p>
            <p><em>Formula:</em> $\sum P(x) = 1$</p>
        </div>
        <div style="flex: 1; background: white; padding: 15px; border: 1px solid #ddd; border-radius: 5px;">
            <h4 style="color: #2980b9;">2. Continuous Distribution</h4>
            <p>Takes on values within a <strong>continuous range</strong> (e.g., height, temperature, time). Probability is measured over an interval.</p>
            <p><em>Formula:</em> $\int f(x)dx = 1$</p>
        </div>
    </div>

    <div class="intuition-box" style="background-color: #fdf2e9; border-left: 4px solid #e67e22; padding: 10px;">
        <strong>Key Difference:</strong> For Discrete variables, we ask: "What is the probability of exactly 2?" For Continuous variables, the probability of an <em>exact</em> single point is technically zero; instead, we ask: "What is the probability of a value falling between 1.9 and 2.1?"
    </div>

    <p><strong>For example:</strong><br>
    $x = 0,1,2$ indicates no head, one head, two head so that $P(X = 0) = 1/4, P(X = 1) = 3/4, P(X = 2) = 1/4$.<br>
    So the probability distribution of x (getting head) is:</p>

    <table class="data-table">
        <thead>
            <tr>
                <th>X</th>
                <th>P(X)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>0</td>
                <td>1/4</td>
            </tr>
            <tr>
                <td>1</td>
                <td>3/4</td>
            </tr>
            <tr>
                <td>2</td>
                <td>1/4</td>
            </tr>
        </tbody>
    </table>

    <h2>Joint Distribution of random variables</h2>
    <p>Let suppose X is a random variable which takes more than one value say $x$ and $y$. Then we say that random variable has a joint probability and Multivaried probability distribution. $P(X = x, Y = y)$.</p>

    <p><strong>Example 1:</strong> X denotes events of tossing two unbiased coins so $X = (x = 0, y=0), X = (x=1,y=0) \dots$ Here $x$ and $y$ are independent variables. We can represent the probabilities in a grid.</p>
    
    <table class="joint-grid">
        <thead>
            <tr>
                <th class="variable-label" rowspan="2">X (Coin 1)</th>
                <th colspan="2" class="variable-label">Y (Coin 2)</th>
                <th rowspan="2" class="marginal-prob">Marginal P(X)</th>
            </tr>
            <tr>
                <th>y = 0 (Tails)</th>
                <th>y = 1 (Heads)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td class="variable-label">x = 0 (Tails)</td>
                <td class="probability-cell">$P(0,0)$<br><strong>1/4</strong></td>
                <td class="probability-cell">$P(0,1)$<br><strong>1/4</strong></td>
                <td class="marginal-prob">1/2</td>
            </tr>
            <tr>
                <td class="variable-label">x = 1 (Heads)</td>
                <td class="probability-cell">$P(1,0)$<br><strong>1/4</strong></td>
                <td class="probability-cell">$P(1,1)$<br><strong>1/4</strong></td>
                <td class="marginal-prob">1/2</td>
            </tr>
            <tr>
                <td class="marginal-prob"><strong>Marginal P(Y)</strong></td>
                <td class="marginal-prob">1/2</td>
                <td class="marginal-prob">1/2</td>
                <td class="marginal-prob" style="background: #eee;"><strong>Sum = 1</strong></td>
            </tr>
        </tbody>
    </table>

    <div class="grid-note">
        <strong>Note:</strong> In a joint distribution grid, the sum of all cell probabilities must equal 1.
    </div>

    <h3>Example 2: Two Dice Rolls</h3>
    <p>When rolling two six-sided dice, the joint distribution shows the probability of every possible pair (e.g., Die 1 = 2 and Die 2 = 5). Since each die is independent, every combination has a probability of $1/6 \times 1/6 = 1/36$.</p>

    <div class="image-container">
    <img src="JointProbTableforTwoDiceThrowing.png" alt="Joint Probability Heat Map">
    <p class="image-caption"><strong>Figure 1:</strong> Joint Probability Heat Map for P(Die 1, Die 2).</p>
</div>



    <p>If we define a new random variable $S$ as the <strong>sum</strong> of the two dice, the distribution changes. While the joint outcomes are uniform, the sum follows a triangular distribution because there are more ways to roll a 7 than a 2 or 12.</p>

    <div class="image-container">
				
    <img src="JDProbabilityDistforTwoDiceThrowing.png" alt="Sum Distribution Chart">
    <p class="image-caption"><strong>Figure 2:</strong> Probability Distribution of the Sum of Two Dice.</p>
</div>
                
    </div>
<h2>Expectations and Covariances</h2>
<p>There are two operations involving probability distribution: (1) Expectations and (2) Covariances.</p>

<div class="example-box">
    <strong>1. Expectation:</strong> The sum of the products of the random variable values and their probabilities.
    <p>For a discrete distribution: $$E[f] = \sum f(x) \cdot p(x)$$ where f(x) is the random variable and p(x) is the corresponding probability</p>
</div>

<p><strong>Example:</strong> Finding the Expectation of the sum of two dice (only for sums less than 7)in the figure 2 above:</p>
<div style="background: #f9f9f9; padding: 10px; border-radius: 5px; font-family: monospace;">
    E[Sum < 7] = (2 * 0.028) + (3 * 0.056) + (4 * 0.083) + (5 * 0.111) + (6 * 0.139) = 2.449
</div>

<div class="example-box" style="border-left-color: #e74c3c;">
    <strong>2. Variance:</strong> The variance of a probability distribution is defined by:
    $$Var[f] = E[(f(x) - E[f])^2]$$
    <p>It provides a measure of how much variability there is in the values of $f(x)$ around its mean $E[f]$.</p>
</div>
    <hr>
    <p><strong>What's Next?</strong> Would you like to explore how to calculate the <em>Expected Value</em> of the sum of these dice rolls?</p>
</div>
<div class="example-box" style="border-left-color: #e74c3c;">
    <strong>3. Covariance:</strong>Covariance involves variances between two or random variables For two random variables x and y, the covariance measures the extent to which the two variables vary together and is defined by
    <p>$$cov[x; y] = E[[x − E[x]][y − E[y]]]= E[xy] − E[x]E[y]$$</p>
</div>
    <hr>
    <p><strong>What's Next?</strong> Would you like to explore how to calculate the <em>Expected Value</em> of the sum of these dice rolls?</p>
</div>


</body>
</html>